{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cmdstanpy import CmdStanModel\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ae64bc3821f1c",
   "metadata": {},
   "source": [
    "## Priors [0-4 pts]\n",
    "- Is it explained why particular priors for parameters were selected [1 pt]\n",
    "- Have prior predictive checks been done for parameters (are parameters simulated from priors make sense) [1 pt]\n",
    "- Have prior predictive checks been done for measurements (are measurements simulated from priors make sense) [1 pt]\n",
    "- How prior parameters were selected [1 pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e089272285a4e02",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model_2_ppc = CmdStanModel(stan_file='stan/model_2_ppc.stan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56ad28d-ced4-4356-83b2-7859f0e4521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_ppc_sim = model_2_ppc.sample(data={'sigma':0.8, 'drive_rating': 2.5}, iter_warmup=1, fixed_param=True, seed=10062022)\n",
    "df_ppc_sim = model_2_ppc_sim.draws_pd()\n",
    "df_ppc_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4be14f30918f441",
   "metadata": {},
   "source": [
    "## Posterior analysis (model 2) [0-4 pts]\n",
    "- were there any issues with the sampling? if there were what kind of ideas for mitigation were used [1 pt]\n",
    "- are the samples from posterior predictive distribution analyzed [1 pt]\n",
    " are the data consistent with posterior predictive samples and is it sufficiently commented (if they are not then is the justification provided)\n",
    "have parameter marginal disrtibutions been analyzed (histograms of individual parametes plus summaries, are they diffuse or concentrated, what can we say about values) [1 pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78c3a2d7ae5264",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T15:02:31.246646Z",
     "start_time": "2025-05-15T15:02:31.242522Z"
    }
   },
   "outputs": [],
   "source": [
    "model_2 = CmdStanModel(stan_file='stan/model_2.stan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b100fa33-f8ef-4fa4-ab11-0fc70c6378a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/processed_data/data.csv')\n",
    "unique_drivers = df['DriverId'].unique()\n",
    "driver_id_map = {driver: idx + 1 for idx, driver in enumerate(unique_drivers)}\n",
    "df['DriverId'] = df['DriverId'].map(driver_id_map)\n",
    "drivers = df['DriverId'].values\n",
    "\n",
    "unique_team = df['TeamId'].unique()\n",
    "team_id_map = {team: idx + 1 for idx, team in enumerate(unique_team)}\n",
    "df['TeamId'] = df['TeamId'].map(team_id_map)\n",
    "teams = df['TeamId'].values\n",
    "\n",
    "unique_engine = df['Engine'].unique()\n",
    "engine_id_map = {engine: idx + 1 for idx, engine in enumerate(unique_engine)}\n",
    "df['Engine'] = df['Engine'].map(engine_id_map)\n",
    "engines = df['Engine'].values\n",
    "\n",
    "unique_season = df['Season'].unique()\n",
    "season_id_map = {season: idx + 1 for idx, season in enumerate(unique_season)}\n",
    "df['Season'] = df['Season'].map(season_id_map)\n",
    "seasons = df['Season'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65698577-0a33-4b8b-a83c-bf3db8e12ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_group(group):\n",
    "    mean = group['Rating'].mean()\n",
    "    std = group['Rating'].std()\n",
    "    group['Rating'] = (group['Rating'] - mean) / std\n",
    "    return group\n",
    "\n",
    "\n",
    "df = df.groupby('Season', group_keys=False, observed=True).apply(standardize_group)\n",
    "ratings = df[\"Rating\"].values\n",
    "df['Position'] = df['Position'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8184922-acd5-44bc-afb0-3420ffdd427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_col = ['DriverId', 'Rating', 'TeamId', 'Engine', 'Season','Position']\n",
    "df = df[order_col]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954be21d-738d-4c1b-95b0-130456a3a802",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_data = {'N': len(df),\n",
    "                'C': len([*team_id_map.values()]),\n",
    "                'E': len([*engine_id_map.values()]),\n",
    "                'D': len([*driver_id_map.values()]),\n",
    "                'Y': len([*season_id_map.values()]),\n",
    "                'driver_rating': ratings,\n",
    "                'engine': engines,\n",
    "                'constructor': teams,                \n",
    "                'driver': drivers,\n",
    "                'year': seasons,\n",
    "                'position': df['Position'] - 1} \n",
    "\n",
    "model_2_fit = model_2.sample(data=model_2_data, seed=25062025,iter_warmup=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82558072-3e7f-43ca-8d21-3408247bc8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "drivers_names = ['hamilton', 'russell', 'tsunoda']\n",
    "fig, axes = plt.subplots(1, len(drivers_names), figsize=(5 * len(drivers_names), 4), sharey=True)\n",
    "\n",
    "n_bins = np.arange(22) - 0.5\n",
    "\n",
    "for d_i, d_name in enumerate(drivers_names):\n",
    "    ax = axes[d_i]\n",
    "    driver_id = driver_id_map[d_name]\n",
    "    results = df[df['DriverId'] == driver_id]\n",
    "    results_idx = results.index\n",
    "\n",
    "    ax.hist((results['Position'] + 1).tolist(),\n",
    "            bins=n_bins,\n",
    "            rwidth=0.9,\n",
    "            histtype='step',\n",
    "            edgecolor='black',\n",
    "            density=True,\n",
    "            label='Observed')\n",
    "\n",
    "    ax.hist(model_2_fit.stan_variable('y_hat').T[results_idx].flatten() + 1,\n",
    "            bins=n_bins,\n",
    "            rwidth=0.9,\n",
    "            color='cornflowerblue',\n",
    "            edgecolor='royalblue',\n",
    "            alpha=0.7,\n",
    "            density=True,\n",
    "            label='Simulated')\n",
    "\n",
    "    ax.set_xticks(range(22))\n",
    "    ax.set_xlim([0, 21])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(d_name.upper() + '\\nfinishing positions (2020â€“2024)', fontsize=11)\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be606eb616f6ae",
   "metadata": {},
   "source": [
    "## Model comaprison [0-4 pts]\n",
    "- Have models been compared using information criteria [1 pt]\n",
    "- Have result for WAIC been discussed (is there a clear winner, or is there an overlap, were there any warnings) [1 pt]\n",
    "- Have result for PSIS-LOO been discussed (is there a clear winner, or is there an overlap, were there any warnings) [1 pt]\n",
    "- Whas the model comparison discussed? Do authors agree with information criteria? Why in your opinion one model better than another [1 pt]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
